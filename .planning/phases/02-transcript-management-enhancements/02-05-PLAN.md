---
phase: 02-transcript-management-enhancements
plan: 05
type: execute
wave: 4
depends_on: []
files_modified:
  - src-tauri/src/commands/transcription.rs
  - src/lib/transcription/types.ts
  - src/components/TranscriptionModal.tsx
autonomous: false
must_haves:
  truths:
    - "App identifies different speakers in transcript (speaker diarization)"
  artifacts:
    - path: "src-tauri/src/commands/transcription.rs"
      provides: "Speaker detection logic integrated into transcription"
      contains: "speaker"
    - path: "src/lib/transcription/types.ts"
      provides: "Speaker‑aware transcript type"
      contains: "speaker"
    - path: "src/components/TranscriptionModal.tsx"
      provides: "Speaker labels displayed in transcript"
      contains: "speaker"
  key_links:
    - from: "src-tauri/src/commands/transcription.rs"
      to: "audio analysis library"
      via: "dependency"
      pattern: "Cargo.toml"
    - from: "src/components/TranscriptionModal.tsx"
      to: "src/lib/transcription/types.ts"
      via: "Speaker type import"
      pattern: "speaker"
---

<objective>
Add speaker diarization to identify different speakers in a conversation transcript.

Purpose: Make multi‑speaker recordings more readable by attributing text to individual speakers.
Output: Speaker detection integrated into transcription pipeline, speaker labels displayed in UI.
</objective>

<execution_context>
@~/.opencode/get-shit-done/workflows/execute-plan.md
@~/.opencode/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md

# Existing transcription pipeline and word timestamps
@src-tauri/src/commands/transcription.rs
@src/lib/transcription/types.ts
</context>

<tasks>

<task type="checkpoint:decision" gate="blocking">
<decision>Choose speaker diarization approach</decision>
<context>
Speaker diarization is a complex audio processing task. Options range from simple silence‑based speaker‑turn detection to full ML‑based diarization models. The choice impacts dependency footprint, accuracy, and implementation complexity.

We need to decide whether to:
1. Implement simple silence‑based speaker turn detection (lightweight, low accuracy)
2. Integrate a Rust ML library (e.g., `pyannote‑rs` wrapper, heavy)
3. Use external service (e.g., AssemblyAI, Deepgram) – adds cost and API dependency
4. Defer to Phase 3 as advanced feature

Considerations:
- Phase 2 already uses OpenAI Whisper API; adding another external API increases cost.
- Local ML model adds significant binary size (~100MB+).
- Simple silence detection may meet the requirement "identifies different speakers" with basic functionality.
</context>
<options>
<option id="silence-detection">
<name>Silence‑based speaker turn detection</name>
<pros>
- No external dependencies beyond audio analysis
- Lightweight, fast
- Can be implemented with existing `symphonia` or `audrey` crates
- Provides basic speaker segmentation
</pros>
<cons>
- Low accuracy: cannot distinguish speakers by voice, only by silence gaps
- May mislabel speakers in overlapping speech
- Requires audio waveform analysis (new domain)
</cons>
</option>
<option id="external-api">
<name>Use external diarization API (AssemblyAI, Deepgram)</name>
<pros>
- High accuracy
- Easy integration (similar to Whisper API)
- Professional‑grade speaker labels
</pros>
<cons>
- Additional cost per minute
- Requires new API key and configuration
- Network dependency
</cons>
</option>
<option id="local-model">
<name>Integrate local ML model (Pyannote via Python subprocess)</name>
<pros>
- Offline capability
- Good accuracy
</pros>
<cons>
- Large binary size
- Python dependency, subprocess complexity
- Slower processing
</cons>
</option>
<option id="defer">
<name>Defer speaker diarization to Phase 3</name>
<pros>
- Focus on other Phase 2 features
- More time for research
</pros>
<cons>
- Phase 2 success criteria not fully met
- User expectation mismatch
</cons>
</option>
</options>
<resume-signal>Select: silence-detection, external-api, local-model, or defer</resume-signal>
</task>

<task type="auto">
<name>Task 2: Implement chosen diarization approach</name>
<files>src-tauri/src/commands/transcription.rs, src-tauri/Cargo.toml</files>
<action>
Based on the decision:

**If silence-detection:**
1. Add audio analysis crate (e.g., `symphonia` or `audrey`) to Cargo.toml.
2. Implement function `detect_speaker_turns(audio_path) -> Vec<SpeakerSegment>` that detects silence gaps and alternates speaker labels.
3. Integrate into transcription pipeline: after receiving Whisper word timestamps, assign each word to the speaker segment that contains its start time.
4. Extend Transcript struct with speaker ID per word (or per segment).

**If external-api:**
1. Choose API provider, add crate (e.g., `reqwest` already present).
2. Implement API call similar to Whisper but for diarization.
3. Merge diarization results with Whisper transcript using time alignment.

**If local-model:**
1. Set up Python subprocess calling Pyannote‑audio.
2. Write Rust wrapper to spawn process and parse output.
3. Integrate with transcription.

**If defer:**
- Skip this task and mark plan as deferred.

Update `Transcript` type to include speaker information (e.g., `words` become `Vec<WordWithSpeaker>`).
</action>
<verify>
- Test with a multi‑speaker audio file.
- Verify speaker labels are assigned.
- Check that transcript still includes all words.
</verify>
<done>
Speaker diarization integrated into transcription pipeline, producing speaker‑labeled transcript.
</done>
</task>

<task type="auto">
<name>Task 3: Display speaker labels in UI</name>
<files>src/components/TranscriptionModal.tsx, src/lib/transcription/types.ts</files>
<action>
1. Update `WordTimestamp` type to include optional `speaker` field (string or number).
2. Modify TranscriptionModal word display to show speaker label:
   - Prefix each word with "Speaker X:" or color‑code.
   - Group consecutive words from same speaker.
3. Add speaker legend (if using colors).
4. Ensure plain text block also includes speaker labels (e.g., "[Speaker 1] Hello world").

Update any other UI that uses transcript (search, export) to handle speaker field.
</action>
<verify>
- Transcribe a multi‑speaker recording.
- Open transcript modal, see speaker labels attached to words.
- Verify export formats include speaker information (if applicable).
</verify>
<done>
Speaker labels visible in transcript UI and preserved across operations.
</done>
</task>

</tasks>

<verification>
1. Record a conversation with two speakers.
2. Transcribe using enhanced pipeline.
3. Open transcript modal – see speaker labels (e.g., "Speaker 1", "Speaker 2") attached to words.
4. Verify speaker assignments make sense (alternating where there are pauses).
</verification>

<success_criteria>
- Transcript includes speaker identification (even if basic).
- UI clearly distinguishes speakers.
- No regression to single‑speaker transcripts.
</success_criteria>

<output>
After completion, create `.planning/phases/02-transcript-management-enhancements/02-05-SUMMARY.md`
</output>